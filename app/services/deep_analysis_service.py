"""
Servi√ßo de An√°lise Profunda usando Refine Chain
Cada chunk √© processado considerando o contexto acumulado dos anteriores
"""
import time
import json
import logging
from typing import Dict, List, Optional, Any, AsyncGenerator
from datetime import datetime
from sqlalchemy.orm import Session
from uuid import UUID

from app.models.deep_analysis import (
    DeepAnalysisJob, 
    DeepAnalysisChunkResult,
    ANALYSIS_TYPE_INFO
)
from app.models.pii import PIIProcessingJob, PIIVault
from app.services.llm_service import get_llm_service

logger = logging.getLogger(__name__)

CHUNK_SIZE_CHARS = 25000

EXTRACTION_PROMPTS = {
    "topic_map": """Analise este trecho de conversa pseudonimizada e extraia de forma estruturada:

## Instru√ß√µes:
1. **T√≥picos discutidos**: Liste cada t√≥pico com t√≠tulo curto
2. **Participantes ativos**: Quem falou neste trecho
3. **Pontos-chave**: Argumentos e ideias principais
4. **Cita√ß√µes relevantes**: Trechos importantes entre aspas
5. **Status**: Para cada t√≥pico marque (resolvido/pendente/em_debate)
6. **Pistas de continuidade**: Identifique t√≥picos que parecem continuar de antes ou para depois

## Formato de sa√≠da (JSON):
```json
{
  "topics": [{"title": "", "description": "", "status": "", "participants": []}],
  "key_points": [""],
  "quotes": [""],
  "continuity_hints": [""]
}
```

## Trecho da conversa:
{chunk_text}""",

    "executive": """Analise este trecho de conversa para um relat√≥rio executivo C-level:

## Extraia:
1. **Decis√µes tomadas**: O que foi decidido?
2. **A√ß√µes acordadas**: Quem vai fazer o qu√™?
3. **Riscos identificados**: Problemas ou preocupa√ß√µes levantadas
4. **Oportunidades**: Ideias positivas ou possibilidades
5. **M√©tricas mencionadas**: N√∫meros, prazos, valores
6. **Pr√≥ximos passos**: O que precisa acontecer?

## Formato JSON:
```json
{
  "decisions": [""],
  "actions": [{"action": "", "responsible": "", "deadline": ""}],
  "risks": [""],
  "opportunities": [""],
  "metrics": [""],
  "next_steps": [""]
}
```

## Trecho:
{chunk_text}""",

    "stakeholder": """Analise os participantes desta conversa:

## Extraia para cada participante:
1. **Identificador**: O pseud√¥nimo usado
2. **Papel aparente**: L√≠der, t√©cnico, decisor, observador, etc.
3. **Posi√ß√£o**: O que defende ou argumenta
4. **N√≠vel de engajamento**: Alto, m√©dio, baixo
5. **Rela√ß√µes**: Com quem concorda/discorda
6. **Cita√ß√µes caracter√≠sticas**: Frases que mostram seu estilo

## Formato JSON:
```json
{
  "stakeholders": [
    {
      "id": "",
      "role": "",
      "position": "",
      "engagement": "",
      "relations": {"agrees_with": [], "disagrees_with": []},
      "quotes": []
    }
  ]
}
```

## Trecho:
{chunk_text}""",

    "timeline": """Extraia a cronologia de decis√µes e eventos desta conversa:

## Identifique:
1. **Eventos/decis√µes**: O que aconteceu ou foi decidido
2. **Momento relativo**: In√≠cio, meio ou fim do trecho
3. **Participantes envolvidos**: Quem participou
4. **Impacto**: Alto, m√©dio ou baixo
5. **Depend√™ncias**: Decis√µes que dependem de outras
6. **Status**: Conclu√≠do, em andamento, pendente

## Formato JSON:
```json
{
  "events": [
    {
      "description": "",
      "position": "",
      "participants": [],
      "impact": "",
      "dependencies": [],
      "status": ""
    }
  ]
}
```

## Trecho:
{chunk_text}"""
}

REFINE_PROMPT = """Voc√™ recebeu uma extra√ß√£o do chunk atual e o contexto acumulado dos chunks anteriores.

## Contexto Acumulado (chunks anteriores):
{accumulated_context}

## Extra√ß√£o do Chunk Atual:
{current_extraction}

## Sua tarefa:
1. **Identifique CONTINUIDADES**: T√≥picos/eventos que j√° apareceram antes e continuam
2. **Identifique NOVIDADES**: Elementos que aparecem pela primeira vez
3. **UNIFIQUE** participantes que aparecem em m√∫ltiplos chunks
4. **MARQUE CONEX√ïES** entre chunks (ex: "decis√£o X do chunk 1 √© referenciada aqui")
5. **ATUALIZE status** de itens anteriores se houver mudan√ßa

## Retorne JSON refinado que integra o contexto anterior com as novas informa√ß√µes:
"""

CONSOLIDATION_PROMPTS = {
    "topic_map": """Consolide todas as extra√ß√µes em um MAPA DE T√ìPICOS DETALHADO.

## Extra√ß√µes de todos os chunks:
{all_extractions}

## Gere um relat√≥rio em Markdown com:

# üó∫Ô∏è Mapa de T√≥picos Detalhado

## üìã Sum√°rio Executivo
(Par√°grafo resumindo os principais temas da conversa)

## üìä Estat√≠sticas
- Total de t√≥picos identificados: X
- T√≥picos principais: X
- Threads identificadas: X

## üéØ T√≥picos Principais (ordenados por relev√¢ncia)

### 1. [Nome do T√≥pico]
**Relev√¢ncia**: üî¥ Alta / üü° M√©dia / üü¢ Baixa
**Status**: Resolvido / Pendente / Em Debate

(Descri√ß√£o detalhada do t√≥pico)

**Participantes envolvidos**:
- PARTICIPANTE_X: (papel/posi√ß√£o)

**Pontos-chave**:
- Ponto 1
- Ponto 2

**Cita√ß√µes relevantes**:
> "cita√ß√£o importante"

**Conex√µes**: Liga-se aos t√≥picos X, Y, Z

---

(Repita para cada t√≥pico principal)

## üîó Mapa de Conex√µes
(Descreva como os t√≥picos se relacionam)

## üí° Insights e Recomenda√ß√µes
(O que essa an√°lise revela? Sugest√µes de a√ß√£o)
""",

    "executive": """Consolide todas as extra√ß√µes em um RELAT√ìRIO EXECUTIVO para C-level.

## Extra√ß√µes:
{all_extractions}

## Gere um relat√≥rio em Markdown:

# üìä Relat√≥rio Executivo

## üéØ Resumo Executivo
(2-3 par√°grafos com os pontos mais importantes para um executivo)

## ‚úÖ Decis√µes Tomadas
| # | Decis√£o | Respons√°vel | Impacto |
|---|---------|-------------|---------|
| 1 | ... | ... | Alto/M√©dio/Baixo |

## üìã Plano de A√ß√£o
| A√ß√£o | Respons√°vel | Prazo | Status |
|------|-------------|-------|--------|
| ... | ... | ... | ... |

## ‚ö†Ô∏è Riscos Identificados
1. **Risco**: (descri√ß√£o)
   - **Probabilidade**: Alta/M√©dia/Baixa
   - **Impacto**: Alto/M√©dio/Baixo
   - **Mitiga√ß√£o sugerida**: ...

## üí° Oportunidades
1. **Oportunidade**: (descri√ß√£o)
   - **Potencial**: Alto/M√©dio/Baixo

## üìà M√©tricas Mencionadas
- M√©trica 1: valor
- M√©trica 2: valor

## ‚û°Ô∏è Pr√≥ximos Passos Recomendados
1. Passo 1
2. Passo 2

## üîç Conclus√£o
(Par√°grafo final com vis√£o geral)
""",

    "stakeholder": """Consolide todas as extra√ß√µes em uma AN√ÅLISE DE STAKEHOLDERS.

## Extra√ß√µes:
{all_extractions}

## Gere um relat√≥rio em Markdown:

# üë• An√°lise de Stakeholders

## üìã Vis√£o Geral
(Par√°grafo sobre a din√¢mica geral do grupo)

## üìä Estat√≠sticas
- Total de participantes: X
- Participantes mais ativos: X
- N√≠veis de engajamento: X alto, X m√©dio, X baixo

## üé≠ Perfil dos Participantes

### PARTICIPANTE_X
**Papel**: (L√≠der/T√©cnico/Decisor/Observador/etc.)
**Engajamento**: üî¥ Alto / üü° M√©dio / üü¢ Baixo
**Posi√ß√£o principal**: (o que defende)

**Caracter√≠sticas**:
- Caracter√≠stica 1
- Caracter√≠stica 2

**Rela√ß√µes**:
- üëç Concorda com: PARTICIPANTE_Y, Z
- üëé Diverge de: PARTICIPANTE_W

**Cita√ß√µes caracter√≠sticas**:
> "cita√ß√£o 1"
> "cita√ß√£o 2"

---

## üó∫Ô∏è Mapa de Influ√™ncia
(Quem influencia quem? Quem s√£o os decisores?)

## ‚ö° Pontos de Tens√£o
(Onde h√° desacordos? Entre quem?)

## ü§ù Alian√ßas Identificadas
(Quem trabalha junto? Quem compartilha vis√µes?)

## üí° Recomenda√ß√µes
(Como trabalhar melhor com esse grupo?)
""",

    "timeline": """Consolide todas as extra√ß√µes em uma TIMELINE DE DECIS√ïES.

## Extra√ß√µes:
{all_extractions}

## Gere um relat√≥rio em Markdown:

# üìÖ Timeline de Decis√µes

## üìã Vis√£o Geral
(Resumo da evolu√ß√£o da conversa)

## üìä Estat√≠sticas
- Total de eventos/decis√µes: X
- Decis√µes conclu√≠das: X
- Decis√µes pendentes: X
- Decis√µes em andamento: X

## üïê Cronologia

### Fase 1: [Nome da Fase]
**Per√≠odo**: In√≠cio da conversa

#### Evento 1.1
- **Descri√ß√£o**: ...
- **Participantes**: PARTICIPANTE_X, Y
- **Impacto**: üî¥ Alto / üü° M√©dio / üü¢ Baixo
- **Status**: ‚úÖ Conclu√≠do / üîÑ Em andamento / ‚è≥ Pendente

#### Evento 1.2
...

---

### Fase 2: [Nome da Fase]
**Per√≠odo**: Meio da conversa
...

---

## üîó Depend√™ncias
```
Decis√£o A ‚Üí Decis√£o B ‚Üí Decis√£o C
            ‚Üò Decis√£o D
```

## ‚è≥ Itens Pendentes
| # | Item | Respons√°vel | Depend√™ncia |
|---|------|-------------|-------------|
| 1 | ... | ... | ... |

## ‚úÖ Decis√µes Conclu√≠das
| # | Decis√£o | Impacto |
|---|---------|---------|
| 1 | ... | ... |

## üí° An√°lise de Padr√µes
(O que a timeline revela sobre o processo de decis√£o do grupo?)
"""
}


class DeepAnalysisService:
    """Servi√ßo para an√°lise profunda com Refine Chain"""
    
    def __init__(self, db: Session):
        self.db = db
        self.llm_service = get_llm_service()
    
    def create_job(
        self,
        pii_job_id: UUID,
        analysis_type: str,
        detail_level: str,
        model: str,
        user_id: UUID,
        organization_id: UUID
    ) -> DeepAnalysisJob:
        """Cria um novo job de an√°lise profunda"""
        
        pii_job = self.db.query(PIIProcessingJob).filter(
            PIIProcessingJob.id == pii_job_id
        ).first()
        
        if not pii_job:
            raise ValueError(f"Job PII {pii_job_id} n√£o encontrado")
        
        if analysis_type not in ANALYSIS_TYPE_INFO:
            raise ValueError(f"Tipo de an√°lise inv√°lido: {analysis_type}")
        
        text_length = len(pii_job.masked_chat_text or "")
        total_chunks = max(1, (text_length + CHUNK_SIZE_CHARS - 1) // CHUNK_SIZE_CHARS)
        
        job = DeepAnalysisJob(
            organization_id=organization_id,
            created_by=user_id,
            pii_job_id=pii_job_id,
            analysis_type=analysis_type,
            detail_level=detail_level,
            model_used=model,
            total_chunks=total_chunks,
            status="pending"
        )
        
        self.db.add(job)
        self.db.commit()
        self.db.refresh(job)
        
        return job
    
    async def process_job(self, job_id: UUID) -> AsyncGenerator[Dict, None]:
        """
        Processa o job usando Refine Chain.
        Yield de progresso para streaming.
        """
        job = self.db.query(DeepAnalysisJob).filter(
            DeepAnalysisJob.id == job_id
        ).first()
        
        if not job:
            raise ValueError(f"Job {job_id} n√£o encontrado")
        
        job.status = "processing"
        job.started_at = datetime.utcnow()
        self.db.commit()
        
        start_time = time.time()
        
        try:
            pii_job = self.db.query(PIIProcessingJob).filter(
                PIIProcessingJob.id == job.pii_job_id
            ).first()
            
            if not pii_job or not pii_job.masked_chat_text:
                raise ValueError("Job PII n√£o possui texto mascarado")
            
            masked_text = pii_job.masked_chat_text
            chunks = self._split_into_chunks(masked_text)
            job.total_chunks = len(chunks)
            self.db.commit()
            
            accumulated_context = ""
            all_extractions = []
            total_tokens = 0
            
            for i, chunk in enumerate(chunks):
                yield {
                    "type": "progress",
                    "step": f"Extraindo chunk {i+1}/{len(chunks)}",
                    "progress": int((i / (len(chunks) + 1)) * 80),
                    "chunk_index": i
                }
                
                chunk_start = time.time()
                
                chunk_result = DeepAnalysisChunkResult(
                    job_id=job.id,
                    chunk_index=i,
                    chunk_content_preview=chunk[:500],
                    status="processing"
                )
                self.db.add(chunk_result)
                self.db.commit()
                
                extraction = await self._extract_chunk(
                    chunk, 
                    job.analysis_type,
                    job.detail_level,
                    job.model_used
                )
                
                chunk_result.extraction_result = extraction
                
                if accumulated_context and i > 0:
                    yield {
                        "type": "progress",
                        "step": f"Refinando chunk {i+1}/{len(chunks)} com contexto",
                        "progress": int((i / (len(chunks) + 1)) * 80) + 5,
                        "chunk_index": i
                    }
                    
                    refined = await self._refine_with_context(
                        extraction,
                        accumulated_context,
                        job.model_used
                    )
                else:
                    refined = extraction
                
                chunk_result.refined_result = refined
                chunk_result.accumulated_context_preview = accumulated_context[:1000] if accumulated_context else None
                chunk_result.processing_time_ms = int((time.time() - chunk_start) * 1000)
                chunk_result.status = "completed"
                
                accumulated_context = self._build_accumulated_context(
                    accumulated_context,
                    refined,
                    i + 1
                )
                
                all_extractions.append(refined)
                
                job.processed_chunks = i + 1
                job.current_step = f"Chunk {i+1}/{len(chunks)} processado"
                self.db.commit()
                
                yield {
                    "type": "chunk_complete",
                    "chunk": i + 1,
                    "total": len(chunks),
                    "progress": int(((i + 1) / (len(chunks) + 1)) * 80)
                }
            
            yield {
                "type": "progress",
                "step": "Consolidando resultados...",
                "progress": 85
            }
            
            final_result = await self._consolidate_results(
                all_extractions,
                job.analysis_type,
                job.detail_level,
                job.model_used
            )
            
            job.final_result = final_result
            job.intermediate_results = all_extractions
            job.status = "completed"
            job.completed_at = datetime.utcnow()
            job.processing_time_seconds = int(time.time() - start_time)
            job.total_tokens_used = total_tokens
            self.db.commit()
            
            yield {
                "type": "complete",
                "progress": 100,
                "result": final_result,
                "processing_time": job.processing_time_seconds
            }
            
        except Exception as e:
            logger.error(f"Erro no processamento deep analysis: {str(e)}")
            job.status = "failed"
            job.error_message = str(e)
            job.completed_at = datetime.utcnow()
            self.db.commit()
            
            yield {
                "type": "error",
                "error": str(e)
            }
    
    def _split_into_chunks(self, text: str) -> List[str]:
        """Divide o texto em chunks menores para an√°lise"""
        chunks = []
        lines = text.split('\n')
        current_chunk = []
        current_size = 0
        
        for line in lines:
            line_size = len(line)
            if current_size + line_size > CHUNK_SIZE_CHARS and current_chunk:
                chunks.append('\n'.join(current_chunk))
                current_chunk = [line]
                current_size = line_size
            else:
                current_chunk.append(line)
                current_size += line_size + 1
        
        if current_chunk:
            chunks.append('\n'.join(current_chunk))
        
        return chunks
    
    async def _extract_chunk(
        self,
        chunk: str,
        analysis_type: str,
        detail_level: str,
        model: str
    ) -> Dict[str, Any]:
        """Extrai informa√ß√µes de um chunk"""
        
        prompt_template = EXTRACTION_PROMPTS.get(analysis_type, EXTRACTION_PROMPTS["topic_map"])
        prompt = prompt_template.format(chunk_text=chunk)
        
        if detail_level == "detalhado":
            prompt += "\n\nSeja extremamente detalhado e inclua todas as informa√ß√µes poss√≠veis."
        elif detail_level == "resumido":
            prompt += "\n\nSeja conciso e foque apenas nos pontos mais importantes."
        
        try:
            response = await self.llm_service.analyze(
                prompt=prompt,
                model=model,
                temperature=0.3,
                max_tokens=2000
            )
            
            try:
                json_match = response
                if "```json" in response:
                    json_match = response.split("```json")[1].split("```")[0]
                elif "```" in response:
                    json_match = response.split("```")[1].split("```")[0]
                
                return json.loads(json_match.strip())
            except:
                return {"raw_response": response}
                
        except Exception as e:
            logger.error(f"Erro na extra√ß√£o: {str(e)}")
            return {"error": str(e)}
    
    async def _refine_with_context(
        self,
        current_extraction: Dict[str, Any],
        accumulated_context: str,
        model: str
    ) -> Dict[str, Any]:
        """Refina a extra√ß√£o atual com o contexto acumulado"""
        
        prompt = REFINE_PROMPT.format(
            accumulated_context=accumulated_context,
            current_extraction=json.dumps(current_extraction, ensure_ascii=False, indent=2)
        )
        
        try:
            response = await self.llm_service.analyze(
                prompt=prompt,
                model=model,
                temperature=0.3,
                max_tokens=2500
            )
            
            try:
                json_match = response
                if "```json" in response:
                    json_match = response.split("```json")[1].split("```")[0]
                elif "```" in response:
                    json_match = response.split("```")[1].split("```")[0]
                
                return json.loads(json_match.strip())
            except:
                return current_extraction
                
        except Exception as e:
            logger.error(f"Erro no refinamento: {str(e)}")
            return current_extraction
    
    def _build_accumulated_context(
        self,
        previous_context: str,
        new_extraction: Dict[str, Any],
        chunk_number: int
    ) -> str:
        """Constr√≥i o contexto acumulado para o pr√≥ximo chunk"""
        
        new_summary = json.dumps(new_extraction, ensure_ascii=False)[:2000]
        
        new_context = f"\n\n### Chunk {chunk_number}:\n{new_summary}"
        
        max_context_size = 8000
        combined = previous_context + new_context
        
        if len(combined) > max_context_size:
            combined = combined[-max_context_size:]
        
        return combined
    
    async def _consolidate_results(
        self,
        all_extractions: List[Dict[str, Any]],
        analysis_type: str,
        detail_level: str,
        model: str
    ) -> str:
        """Consolida todas as extra√ß√µes em um relat√≥rio final"""
        
        prompt_template = CONSOLIDATION_PROMPTS.get(analysis_type, CONSOLIDATION_PROMPTS["topic_map"])
        
        extractions_text = "\n\n".join([
            f"### Chunk {i+1}:\n{json.dumps(ext, ensure_ascii=False, indent=2)}"
            for i, ext in enumerate(all_extractions)
        ])
        
        prompt = prompt_template.format(all_extractions=extractions_text)
        
        if detail_level == "detalhado":
            prompt += "\n\nGere um relat√≥rio extremamente detalhado e abrangente."
        elif detail_level == "resumido":
            prompt += "\n\nGere um relat√≥rio conciso focando nos pontos mais importantes."
        
        try:
            response = await self.llm_service.analyze(
                prompt=prompt,
                model=model,
                temperature=0.5,
                max_tokens=4000
            )
            
            return response
            
        except Exception as e:
            logger.error(f"Erro na consolida√ß√£o: {str(e)}")
            return f"Erro na consolida√ß√£o: {str(e)}"
    
    def get_job(self, job_id: UUID) -> Optional[DeepAnalysisJob]:
        """Retorna um job pelo ID"""
        return self.db.query(DeepAnalysisJob).filter(
            DeepAnalysisJob.id == job_id
        ).first()
    
    def list_jobs(self, organization_id: UUID, limit: int = 20) -> List[DeepAnalysisJob]:
        """Lista jobs da organiza√ß√£o"""
        return self.db.query(DeepAnalysisJob).filter(
            DeepAnalysisJob.organization_id == organization_id
        ).order_by(DeepAnalysisJob.created_at.desc()).limit(limit).all()
    
    async def deanonymize_result(
        self,
        job_id: UUID
    ) -> str:
        """Re-hidrata o resultado usando o vault do job PII"""
        
        job = self.get_job(job_id)
        if not job or not job.final_result:
            raise ValueError("Job n√£o encontrado ou sem resultado")
        
        vault = self.db.query(PIIVault).filter(
            PIIVault.job_id == job.pii_job_id
        ).first()
        
        if not vault or not vault.deanonymizer_mapping:
            raise ValueError("Vault n√£o dispon√≠vel para este job")
        
        result = job.final_result
        
        mappings = sorted(
            vault.deanonymizer_mapping.items(),
            key=lambda x: len(x[0]),
            reverse=True
        )
        
        import re
        for pseudo, original in mappings:
            escaped = re.escape(pseudo)
            pattern = rf'(?<![A-Za-z0-9_]){escaped}(?![A-Za-z0-9_])'
            result = re.sub(pattern, original, result)
            
            clean_pseudo = pseudo.replace('[', '').replace(']', '')
            if clean_pseudo != pseudo:
                escaped_clean = re.escape(clean_pseudo)
                pattern_clean = rf'(?<![A-Za-z0-9_]){escaped_clean}(?![A-Za-z0-9_])'
                result = re.sub(pattern_clean, original, result)
        
        return result
