"""
Tarefas Celery para processamento de PII em chunks
Arquivo: app/tasks/pii_tasks.py
"""

import logging
import time
import asyncio
import re
from datetime import datetime, timedelta
from typing import List, Dict, Optional

from app.core.database import SessionLocal
from app.models.pii import PIIAnalysis, PIIAnalysisChunk, PIIProcessingJob
from app.services.llm_service import get_llm_service

logger = logging.getLogger(__name__)

MAX_RETRIES = 3
RETRY_DELAY = 5
RATE_LIMIT_BASE_DELAY = 35

MODEL_CHUNK_SIZES = {
    "gpt-3.5-turbo": {"size": 12000, "overlap": 2000},
    "gpt-4-turbo": {"size": 60000, "overlap": 10000},
    "gpt-4o": {"size": 60000, "overlap": 10000},
    "gpt-4o-mini": {"size": 60000, "overlap": 10000},
    "claude-3-opus": {"size": 80000, "overlap": 15000},
    "claude-3-sonnet": {"size": 80000, "overlap": 15000},
    "claude-3-haiku": {"size": 40000, "overlap": 8000},
}
DEFAULT_CHUNK_SIZE = 60000
DEFAULT_CHUNK_OVERLAP = 10000


def extract_rate_limit_info(error_message: str) -> Dict:
    """Extract rate limit info from OpenAI error message."""
    info = {
        "is_rate_limit": False,
        "wait_seconds": RATE_LIMIT_BASE_DELAY,
        "limit": None,
        "used": None,
        "requested": None
    }
    
    if "429" in str(error_message) or "rate_limit" in str(error_message).lower():
        info["is_rate_limit"] = True
        
        wait_match = re.search(r'try again in (\d+(?:\.\d+)?)\s*s', str(error_message))
        if wait_match:
            info["wait_seconds"] = int(float(wait_match.group(1))) + 5
        
        limit_match = re.search(r'Limit (\d+)', str(error_message))
        if limit_match:
            info["limit"] = int(limit_match.group(1))
            
        used_match = re.search(r'Used (\d+)', str(error_message))
        if used_match:
            info["used"] = int(used_match.group(1))
            
        requested_match = re.search(r'Requested (\d+)', str(error_message))
        if requested_match:
            info["requested"] = int(requested_match.group(1))
    
    return info


def update_analysis_timing(db, analysis, chunk_times: List[int]):
    """Update analysis timing estimates based on chunk processing times."""
    if not chunk_times:
        return
    
    avg_time_ms = sum(chunk_times) / len(chunk_times)
    analysis.avg_chunk_time_ms = str(int(avg_time_ms))
    
    total_chunks = int(analysis.total_chunks or 0)
    completed = int(analysis.completed_chunks or 0)
    remaining = total_chunks - completed
    
    if remaining > 0 and avg_time_ms > 0:
        remaining_ms = remaining * avg_time_ms
        remaining_seconds = remaining_ms / 1000
        analysis.estimated_completion = datetime.utcnow() + timedelta(seconds=remaining_seconds)
    
    db.commit()


TASK_PROMPTS = {
    "sentiment": {
        "chunk": """Voc√™ √© um psic√≥logo organizacional analisando din√¢micas de grupo.

## CONTEXTO
Esta √© a PARTE {chunk_num} de {total_chunks} de uma conversa de WhatsApp profissional.

## FRAMEWORK DE AN√ÅLISE

### Dimens√µes do Sentimento
1. **Val√™ncia**: positivo / negativo / neutro / misto
2. **Intensidade**: forte / moderada / leve
3. **Dire√ß√£o**: geral / entre pessoas espec√≠ficas / sobre t√≥pico espec√≠fico

### Indicadores

**POSITIVO**: "√≥timo", "perfeito", "adorei", "excelente", "parab√©ns", emojis üëèüéâ‚úÖüí™üôåüòä
**NEGATIVO**: "problema", "dif√≠cil", "infelizmente", "p√©ssimo", "absurdo", emojis üò§üò¢üò°üëé
**TENS√ÉO**: discord√¢ncia expl√≠cita, sil√™ncio ap√≥s confronto, mudan√ßa brusca de assunto
**NEUTRO**: compartilhamento de fatos, perguntas objetivas, links sem coment√°rio

## INSTRU√á√ïES
1. Identifique o sentimento geral desta parte
2. Mapeie a evolu√ß√£o temporal (in√≠cio, meio, fim)
3. Analise o sentimento por participante
4. Identifique tens√µes entre pessoas
5. Destaque momentos-chave (picos positivos/negativos)
6. Cite evid√™ncias textuais para cada observa√ß√£o

## FORMATO DE RESPOSTA

### Sentimento Geral
**Val√™ncia**: [positivo/negativo/neutro/misto]
**Intensidade**: [forte/moderada/leve]

### Evolu√ß√£o Temporal
- **In√≠cio**: [sentimento] - "[cita√ß√£o]"
- **Meio**: [sentimento] - "[cita√ß√£o]"
- **Fim**: [sentimento] - "[cita√ß√£o]"

### Por Participante
[Para cada participante ativo, descrever sentimento predominante com cita√ß√µes]

### Tens√µes Identificadas
[Descrever conflitos entre pessoas com evid√™ncias]

### Momentos-Chave
[Picos emocionais positivos e negativos com cita√ß√µes]

### Clima Geral
[Uma frase resumindo o clima emocional desta parte]""",
        "consolidate": """Voc√™ √© um diretor de RH consolidando uma an√°lise de clima de uma conversa longa.

## DADOS RECEBIDOS
{chunk_results}

## TAREFA
Criar RELAT√ìRIO DE CLIMA EMOCIONAL consolidado.

## REGRAS
1. DEDUPLICAR: Tens√µes repetidas ‚Üí manter descri√ß√£o mais completa
2. EVOLU√á√ÉO: Mostrar como o clima mudou ao longo da conversa
3. EVID√äNCIAS: Incluir cita√ß√µes para cada conclus√£o
4. PRIORIZAR: Tens√µes n√£o resolvidas > resolvidas

## ESTRUTURA OBRIGAT√ìRIA

# üí≠ An√°lise de Sentimento

## üìå Resumo Executivo
[2-3 frases sobre o clima geral]

## üé≠ Sentimento Geral
| Dimens√£o | Avalia√ß√£o | Confian√ßa |
|----------|-----------|-----------|
| Val√™ncia | [pos/neg/neutro/misto] | [alta/m√©dia/baixa] |
| Intensidade | [forte/moderada/leve] | [alta/m√©dia/baixa] |

## üìà Evolu√ß√£o Temporal
```
[In√≠cio] ‚ûú [Meio] ‚ûú [Fim]
[emoji] ‚ûú [emoji] ‚ûú [emoji]
```
**An√°lise**: [como e por que o sentimento evoluiu]

## üë• An√°lise por Participante
| Participante | Sentimento | Momentos Destaque |
|--------------|------------|-------------------|
| [nome] | [descri√ß√£o] | "[cita√ß√£o positiva]" / "[cita√ß√£o negativa]" |

## ‚ö†Ô∏è Tens√µes Identificadas
### [Tens√£o 1]
- **Entre**: [pessoa1] e [pessoa2]
- **Sobre**: [t√≥pico]
- **Intensidade**: [alta/m√©dia/baixa]
- **Resolvida**: [sim/n√£o]
- **Evid√™ncia**: "[cita√ß√£o]"

## üåü Momentos de Destaque
### Picos Positivos
- [descri√ß√£o] - "[cita√ß√£o]"

### Picos Negativos
- [descri√ß√£o] - "[cita√ß√£o]"

## üí° Recomenda√ß√µes
[Sugest√µes para melhorar o clima do grupo]"""
    },
    "summary": {
        "chunk": """Voc√™ √© um analista de comunica√ß√£o corporativa com 15 anos de experi√™ncia em grupos de WhatsApp empresariais.

## CONTEXTO
Esta √© a PARTE {chunk_num} de {total_chunks} de uma conversa de WhatsApp.

## TAREFA
Analise esta parte e extraia informa√ß√µes estruturadas.

## INSTRU√á√ïES ESPEC√çFICAS

### Participantes
- Liste participantes que aparecem nesta parte
- Identifique papel/expertise aparente
- Note frequ√™ncia: muito ativo / moderado / pontual

### T√≥picos
Para cada t√≥pico SUBSTANTIVO discutido:
- Ignore: sauda√ß√µes, "bom dia", emojis isolados, "<m√≠dia oculta>"
- Inclua: discuss√µes com 3+ mensagens sobre o mesmo assunto
- Classifique: t√©cnico / neg√≥cio / social / administrativo

### Decis√µes e Compromissos
APENAS inclua se houver:
- Verbo de compromisso: "vou", "fico de", "me comprometo"
- Confirma√ß√£o: "fechado", "combinado", "ok, fa√ßo"
- Prazo mencionado

### Informa√ß√µes Valiosas
- Links compartilhados (URL + contexto)
- Eventos mencionados (nome + data)
- Documentos referenciados
- Dados/estat√≠sticas citados

## FORMATO DE RESPOSTA

### Participantes desta Parte
| Nome | Papel/Expertise | Atividade |
|------|-----------------|-----------|
| [nome] | [descri√ß√£o] | [alta/m√©dia/baixa] |

### T√≥picos Discutidos
**[Nome do T√≥pico]**
- Tipo: [t√©cnico/neg√≥cio/social/admin]
- Status: [resolvido/pendente/em debate]
- Participantes: [nomes]
- Resumo: [3-4 frases]
- Mensagens relevantes: [quantidade]

### Decis√µes e Compromissos
| Decis√£o | Respons√°vel | Prazo | Evid√™ncia |
|---------|-------------|-------|-----------|
| [descri√ß√£o] | [nome] | [data] | "[cita√ß√£o exata]" |

### Informa√ß√µes Valiosas
- [Tipo]: [conte√∫do] - Contexto: [explica√ß√£o]

### Pontos de Tens√£o
- [descri√ß√£o de conflitos ou desacordos]

### Resumo para Contexto
[3-4 frases resumindo esta parte para servir de contexto]""",
        "consolidate": """Voc√™ √© um diretor executivo que precisa de uma s√≠ntese clara de uma longa conversa de WhatsApp.

## DADOS RECEBIDOS
{chunk_results}

## TAREFA
Criar RELAT√ìRIO EXECUTIVO que seja:
- Acion√°vel (o que fazer com essa informa√ß√£o?)
- Conciso (m√°ximo 1500 palavras)
- Priorizado (mais importante primeiro)

## REGRAS DE CONSOLIDA√á√ÉO

### Deduplica√ß√£o
- Mesmo t√≥pico em m√∫ltiplos chunks ‚Üí unifique, mantenha evolu√ß√£o
- Mesmo participante ‚Üí consolide informa√ß√µes

### Prioriza√ß√£o
- Decis√µes tomadas > Discuss√µes em andamento > Men√ß√µes breves
- Com prazo > Sem prazo
- Com respons√°vel > Sem respons√°vel

## ESTRUTURA OBRIGAT√ìRIA

# üìã An√°lise da Conversa

## üìå TL;DR
[2-3 frases capturando o essencial. Um executivo ocupado leria S√ì isso.]

## üë• Participantes-Chave
| Nome | Papel/Expertise | Atividade |
|------|-----------------|-----------|
| [nome] | [descri√ß√£o] | üü¢ Alto / üü° M√©dio / üî¥ Baixo |

## üìÖ Linha do Tempo
- **[Data/Per√≠odo]**: [Evento/Marco importante]
- **[Data/Per√≠odo]**: [Evento/Marco importante]

## üéØ T√≥picos Principais

### 1. [T√≥pico mais discutido]
- **Status**: ‚úÖ Resolvido / ‚è≥ Pendente / üí¨ Em debate
- **Resumo**: [3-4 frases]
- **Participantes-chave**: [nomes]
- **Conclus√£o/Pr√≥ximos passos**: [se houver]

### 2. [Segundo t√≥pico]
[mesma estrutura]

## ‚úÖ Decis√µes e Compromissos
| A√ß√£o | Respons√°vel | Prazo | Status | Evid√™ncia |
|------|-------------|-------|--------|-----------|
| [descri√ß√£o] | [nome] | [data] | ‚è≥/‚úÖ | "[cita√ß√£o]" |

## ‚ö†Ô∏è Pend√™ncias Cr√≠ticas
1. [Algo que precisa de aten√ß√£o/decis√£o]
2. [...]

## üö® Alertas
- [Tens√µes identificadas]
- [Riscos mencionados]
- [Urg√™ncias]

## üìé Recursos Mencionados
- [Links, documentos, eventos com contexto]"""
    },
    "topics": {
        "chunk": """Voc√™ √© um analista de conte√∫do especializado em mapear discuss√µes em grupos profissionais.

## CONTEXTO
Esta √© a PARTE {chunk_num} de {total_chunks} de uma conversa de WhatsApp.

## O QUE √â UM T√ìPICO

### ‚úÖ INCLUIR
- Assunto discutido por 2+ pessoas com 3+ mensagens
- Tem subst√¢ncia (informa√ß√£o, debate, decis√£o)
- Exemplos: "Implementa√ß√£o de chatbot", "Evento de networking", "Regulamenta√ß√£o de IA"

### ‚ùå IGNORAR
- Sauda√ß√µes: "bom dia", "oi pessoal"
- Meta-conversa: "o grupo t√° quieto", "algu√©m a√≠?"
- M√≠dia sem contexto: "<m√≠dia oculta>" sozinha
- Rea√ß√µes isoladas: emojis, "kkk", "haha"

## TAXONOMIA DE T√ìPICOS
- **tecnico**: C√≥digo, ferramentas, arquitetura, bugs, implementa√ß√£o
- **negocio**: Estrat√©gia, mercado, clientes, vendas, parcerias
- **evento**: Meetups, confer√™ncias, webinars, encontros
- **regulatorio**: Leis, compliance, √©tica, pol√≠ticas
- **carreira**: Vagas, oportunidades, networking, desenvolvimento
- **social**: Conversas pessoais, humor, off-topic
- **administrativo**: Regras do grupo, organiza√ß√£o, avisos

## M√âTRICAS DE RELEV√ÇNCIA
**ALTA**: 10+ mensagens OU decis√£o tomada OU m√∫ltiplos participantes engajados
**M√âDIA**: 5-10 mensagens OU debate sem conclus√£o
**BAIXA**: 3-5 mensagens OU men√ß√£o passageira

## FORMATO DE RESPOSTA

### T√≥picos Identificados

**[Nome do T√≥pico]**
- ID: T{chunk_num}.[sequ√™ncia]
- Categoria: [t√©cnico/neg√≥cio/evento/regulat√≥rio/carreira/social/admin]
- Relev√¢ncia: [üî¥ Alta / üü° M√©dia / üü¢ Baixa]
- Mensagens: [quantidade estimada]
- Participantes: [nomes]
- Status: [‚úÖ Resolvido / ‚è≥ Pendente / üí¨ Em debate / ‚ÑπÔ∏è Informativo]
- Descri√ß√£o: [3-4 frases]
- Cita√ß√£o-chave: "[frase que captura a ess√™ncia]"
- Sentimento: [positivo/negativo/neutro/controverso]

### Conex√µes entre T√≥picos
[Descrever como os t√≥picos se relacionam]

### Resumo para Contexto
[2-3 frases para servir de contexto para pr√≥ximas partes]""",
        "consolidate": """Voc√™ √© um curador de conhecimento criando um mapa de t√≥picos de uma conversa longa.

## DADOS RECEBIDOS
{chunk_results}

## TAREFA
Criar MAPA DE T√ìPICOS consolidado e hierarquizado.

## REGRAS

### Agrupamento
- T√≥picos relacionados ‚Üí agrupar sob tema pai
- Ex: "Chatbot para vendas" + "Chatbot para suporte" ‚Üí "Implementa√ß√£o de Chatbots"

### Deduplica√ß√£o
- Mesmo t√≥pico em chunks diferentes ‚Üí unificar
- Manter evolu√ß√£o temporal

## ESTRUTURA OBRIGAT√ìRIA

# üó∫Ô∏è Mapa de T√≥picos

## üìä Vis√£o Geral
- **Total de t√≥picos identificados**: [X]
- **Temas principais**: [Y]

## üìà Distribui√ß√£o por Categoria
| Categoria | Quantidade | % do Total |
|-----------|------------|------------|
| T√©cnico | [X] | [Y%] |
| Neg√≥cio | [X] | [Y%] |
| [outros] | [X] | [Y%] |

---

## üéØ Temas Principais

### 1. [Tema Principal]
**Relev√¢ncia**: üî¥ Alta | **Status Geral**: ‚è≥ Em andamento

#### Contexto
[2-3 frases sobre o tema]

#### T√≥picos Relacionados

##### 1.1 [T√≥pico]
- **Status**: ‚úÖ Resolvido / ‚è≥ Pendente / üí¨ Em debate
- **Participantes-chave**: [nomes]
- **Resumo**: [3-4 frases]
- **Cita√ß√£o-chave**: "[frase]"
- **Conclus√£o**: [se houver]

### 2. [Segundo Tema Principal]
[mesma estrutura]

---

## üìå T√≥picos N√£o Resolvidos
1. [T√≥pico] - √öltima discuss√£o sobre [assunto]
2. [T√≥pico] - Aguardando [o qu√™]

## üí° Insights
- [Padr√£o observado]
- [Tend√™ncia identificada]"""
    },
    "intent": {
        "chunk": """Voc√™ √© um analista de comunica√ß√£o classificando inten√ß√µes em mensagens.

## CONTEXTO
Esta √© a PARTE {chunk_num} de {total_chunks} de uma conversa de WhatsApp.

## TAXONOMIA DE INTEN√á√ïES

### Prim√°rias
- **informar**: Compartilhar not√≠cia, dado, conhecimento (sem pedir nada)
- **perguntar**: Buscar informa√ß√£o, tirar d√∫vida
- **solicitar**: Pedir a√ß√£o espec√≠fica de algu√©m
- **oferecer**: Disponibilizar ajuda, recurso, tempo
- **decidir**: Propor ou confirmar decis√£o
- **debater**: Argumentar posi√ß√£o, discordar, defender ponto

### Secund√°rias
- **networking**: Conectar pessoas, apresentar
- **promover**: Divulgar evento, produto, servi√ßo pr√≥prio
- **reclamar**: Expressar insatisfa√ß√£o
- **agradecer**: Reconhecer contribui√ß√£o
- **socializar**: Manter relacionamento (sauda√ß√µes, humor)
- **moderar**: Gerenciar grupo (regras, organiza√ß√£o)

## AN√ÅLISE SOLICITADA
1. Identifique a inten√ß√£o predominante desta parte
2. Mapeie a distribui√ß√£o de inten√ß√µes (%)
3. Analise inten√ß√£o por participante
4. Identifique fluxos de inten√ß√£o (pergunta ‚Üí resposta)
5. Liste inten√ß√µes n√£o atendidas

## FORMATO DE RESPOSTA

### Inten√ß√£o Predominante
**Tipo**: [inten√ß√£o]
**Confian√ßa**: [alta/m√©dia/baixa]
**Evid√™ncia**: "[cita√ß√£o representativa]"

### Distribui√ß√£o de Inten√ß√µes
| Inten√ß√£o | Percentual |
|----------|------------|
| Informar | [X%] |
| Perguntar | [X%] |
| Solicitar | [X%] |
| [outros] | [X%] |

### Por Participante
| Nome | Inten√ß√£o Principal | Perfil | Exemplos |
|------|-------------------|--------|----------|
| [nome] | [inten√ß√£o] | [contribuidor/questionador/moderador/observador/promotor] | "[cita√ß√£o]" |

### Fluxos de Inten√ß√£o
- [Pergunta de Jo√£o] ‚Üí [Resposta de Maria] ‚Üí [Resultado: resolvido/pendente]

### Inten√ß√µes N√£o Atendidas
- [Tipo]: [descri√ß√£o] - De: [pessoa] - Status: [sem resposta/parcialmente atendida]""",
        "consolidate": """Voc√™ √© um analista s√™nior consolidando um mapa de inten√ß√µes de uma conversa longa.

## DADOS RECEBIDOS
{chunk_results}

## TAREFA
Determinar o MAPA DE INTEN√á√ïES consolidado.

## ESTRUTURA OBRIGAT√ìRIA

# üéØ An√°lise de Inten√ß√µes

## üìå Resumo Executivo
[2-3 frases sobre as inten√ß√µes dominantes]

## üèÜ Inten√ß√£o Principal
**Tipo**: [inten√ß√£o]
**Evid√™ncia**: "[cita√ß√£o]"
**An√°lise**: [por que esta √© a inten√ß√£o dominante]

## üìä Distribui√ß√£o Geral
| Inten√ß√£o | % | Tend√™ncia |
|----------|---|-----------|
| [inten√ß√£o] | [X%] | ‚Üë Crescente / ‚Üì Decrescente / ‚Üí Est√°vel |

## üë• Perfil por Participante
| Participante | Papel | Inten√ß√£o Principal | Contribui√ß√£o |
|--------------|-------|-------------------|--------------|
| [nome] | [contribuidor/questionador/moderador] | [inten√ß√£o] | [descri√ß√£o] |

## üîÑ Fluxos de Inten√ß√£o
### Perguntas e Respostas
| Pergunta | De | Resposta | Por | Status |
|----------|-----|----------|-----|--------|
| [resumo] | [nome] | [resumo] | [nome] | ‚úÖ/‚è≥ |

## ‚ö†Ô∏è Inten√ß√µes N√£o Atendidas
1. **[Tipo]**: [descri√ß√£o]
   - De: [pessoa]
   - Desde: [momento/parte]
   - Impacto: [alto/m√©dio/baixo]

## üí° Recomenda√ß√µes
[Sugest√µes para melhorar o fluxo de inten√ß√µes]"""
    },
    "quality": {
        "chunk": """Voc√™ √© um consultor de comunica√ß√£o corporativa avaliando qualidade de intera√ß√µes.

## CONTEXTO
Esta √© a PARTE {chunk_num} de {total_chunks} de uma conversa de WhatsApp.

## CRIT√âRIOS DE AVALIA√á√ÉO (1-10)

### 1. Clareza (as mensagens s√£o compreens√≠veis?)
- 9-10: Mensagens claras, bem estruturadas, sem ambiguidade
- 7-8: Majoritariamente claras, algumas precisam contexto
- 5-6: Mistura de claras e confusas
- 3-4: Frequentemente confusas ou incompletas
- 1-2: Incompreens√≠veis, muito fragmentadas

### 2. Profissionalismo (tom adequado ao contexto?)
- 9-10: Tom consistentemente profissional e respeitoso
- 7-8: Profissional com momentos de informalidade apropriada
- 5-6: Mistura de profissional e casual
- 3-4: Muito informal ou ocasionalmente inadequado
- 1-2: Inadequado, ofensivo ou muito desleixado

### 3. Efici√™ncia (objetividade nas mensagens?)
- 9-10: Direto ao ponto, sem redund√¢ncia
- 7-8: Majoritariamente eficiente
- 5-6: Algumas mensagens poderiam ser mais concisas
- 3-4: Muita redund√¢ncia ou dispers√£o
- 1-2: Extremamente prolixo ou desorganizado

### 4. Engajamento (participa√ß√£o e intera√ß√£o?)
- 9-10: Alto engajamento, m√∫ltiplos participantes ativos
- 7-8: Bom engajamento, algumas conversas bilaterais
- 5-6: Engajamento moderado
- 3-4: Pouco engajamento, muitas mensagens sem resposta
- 1-2: Quase mon√≥logo ou grupo inativo

### 5. Resolu√ß√£o (problemas s√£o resolvidos?)
- 9-10: Quest√µes levantadas s√£o respondidas/resolvidas
- 7-8: Maioria resolvida, algumas pendentes
- 5-6: Metade resolvida
- 3-4: Maioria fica pendente
- 1-2: Nada √© resolvido, conversas abandonadas

## FORMATO DE RESPOSTA

### Notas desta Parte

| Crit√©rio | Nota | Justificativa |
|----------|------|---------------|
| Clareza | [X]/10 | [explica√ß√£o] |
| Profissionalismo | [X]/10 | [explica√ß√£o] |
| Efici√™ncia | [X]/10 | [explica√ß√£o] |
| Engajamento | [X]/10 | [explica√ß√£o] |
| Resolu√ß√£o | [X]/10 | [explica√ß√£o] |

**M√©dia**: [X.X]/10

### Mensagens Exemplares
| Autor | Mensagem | Por que √© boa |
|-------|----------|---------------|
| [nome] | "[cita√ß√£o]" | [explica√ß√£o] |

### Mensagens Problem√°ticas
| Autor | Mensagem | Problema | Sugest√£o |
|-------|----------|----------|----------|
| [nome] | "[cita√ß√£o]" | [problema] | [como melhorar] |

### Pontos Fortes
- [ponto forte com evid√™ncia]

### Oportunidades de Melhoria
- [oportunidade com sugest√£o]""",
        "consolidate": """Voc√™ √© um consultor s√™nior criando um relat√≥rio de qualidade de comunica√ß√£o.

## DADOS RECEBIDOS
{chunk_results}

## TAREFA
Criar RELAT√ìRIO DE QUALIDADE consolidado.

## ESTRUTURA OBRIGAT√ìRIA

# üìä Relat√≥rio de Qualidade da Comunica√ß√£o

## üìå Resumo Executivo
[2-3 frases sobre a qualidade geral]

## üèÜ Nota Geral: [X.X]/10

## üìà Detalhamento por Crit√©rio

| Crit√©rio | Nota | Tend√™ncia | An√°lise |
|----------|------|-----------|---------|
| Clareza | [X]/10 | ‚Üë‚Üì‚Üí | [resumo] |
| Profissionalismo | [X]/10 | ‚Üë‚Üì‚Üí | [resumo] |
| Efici√™ncia | [X]/10 | ‚Üë‚Üì‚Üí | [resumo] |
| Engajamento | [X]/10 | ‚Üë‚Üì‚Üí | [resumo] |
| Resolu√ß√£o | [X]/10 | ‚Üë‚Üì‚Üí | [resumo] |

## üåü Pontos Fortes
1. **[Ponto]**: [descri√ß√£o com evid√™ncia]
2. **[Ponto]**: [descri√ß√£o com evid√™ncia]

## ‚ö†Ô∏è Oportunidades de Melhoria
1. **[√Årea]**: [descri√ß√£o do problema]
   - **Impacto**: [alto/m√©dio/baixo]
   - **Sugest√£o**: [como melhorar]

## üëë Destaques Positivos
| Participante | Contribui√ß√£o | Exemplo |
|--------------|--------------|---------|
| [nome] | [descri√ß√£o] | "[cita√ß√£o]" |

## üìã Recomenda√ß√µes Pr√°ticas
1. [Recomenda√ß√£o acion√°vel]
2. [Recomenda√ß√£o acion√°vel]
3. [Recomenda√ß√£o acion√°vel]"""
    },
    "action_items": {
        "chunk": """Voc√™ √© um gerente de projetos PMI-certificado especializado em extrair compromissos de comunica√ß√£o informal.

## CONTEXTO
Esta √© a PARTE {chunk_num} de {total_chunks} de uma conversa de WhatsApp.

## CLASSIFICA√á√ÉO DE A√á√ïES

### üü¢ COMPROMISSO FIRME (alta confian√ßa)
Crit√©rios - TODOS devem estar presentes:
- Verbo de primeira pessoa: "vou", "fa√ßo", "fico de", "assumo"
- OU confirma√ß√£o expl√≠cita: "ok", "fechado", "combinado", "pode deixar"
- A√ß√£o espec√≠fica e verific√°vel

Exemplos:
‚úÖ "Fico de mandar o relat√≥rio at√© sexta" ‚Üí compromisso firme
‚úÖ "Ok, eu reviso amanh√£" ‚Üí compromisso firme
‚ùå "Seria bom algu√©m revisar" ‚Üí N√ÉO √© compromisso

### üü° SOLICITA√á√ÉO (m√©dia confian√ßa)
Crit√©rios:
- Pedido direcionado a pessoa espec√≠fica
- Usa @men√ß√£o ou nome
- Aguarda confirma√ß√£o

### üü† SUGEST√ÉO (baixa confian√ßa)
Crit√©rios:
- Sem respons√°vel definido
- Linguagem condicional: "seria bom", "precisamos", "algu√©m poderia"

### ‚ö™ IGNORAR
- Perguntas ret√≥ricas
- Desejos sem a√ß√£o: "queria muito que..."
- Coment√°rios sobre a√ß√µes de terceiros

## PRIORIZA√á√ÉO
**ALTA**: Prazo expl√≠cito ‚â§ 7 dias OU palavras "urgente", "cr√≠tico", "bloqueado"
**M√âDIA**: Prazo expl√≠cito > 7 dias OU sem prazo mas com respons√°vel
**BAIXA**: Sem prazo e sem respons√°vel claro

## FORMATO DE RESPOSTA

### A√ß√µes Identificadas

**A√ß√£o A{chunk_num}.1**
- Descri√ß√£o: [verbo + objeto + contexto]
- Respons√°vel: [nome ou "indefinido"]
- Prazo: [data espec√≠fica ou "n√£o mencionado"]
- Prioridade: [üî¥ Alta / üü° M√©dia / üü¢ Baixa]
- Tipo: [compromisso/solicita√ß√£o/sugest√£o]
- Confian√ßa: [alta/m√©dia/baixa]
- Evid√™ncia: "[cita√ß√£o EXATA]"
- Contexto: [por que essa a√ß√£o surgiu]
- Depend√™ncia: [outra a√ß√£o ou "nenhuma"]

### Aguardando Confirma√ß√£o
[Lista de solicita√ß√µes feitas mas sem resposta]

### Resumo desta Parte
- Total de a√ß√µes: [X]
- Compromissos firmes: [Y]
- Solicita√ß√µes: [Z]
- Sugest√µes: [W]""",
        "consolidate": """Voc√™ √© um PMO (Project Management Officer) consolidando a√ß√µes de uma conversa longa.

## DADOS RECEBIDOS
{chunk_results}

## TAREFA
Criar PLANO DE A√á√ÉO consolidado e deduplicado.

## REGRAS DE CONSOLIDA√á√ÉO

### Deduplica√ß√£o
- Mesma a√ß√£o mencionada em chunks diferentes ‚Üí manter a mais recente/completa
- A√ß√£o que evoluiu (de sugest√£o para compromisso) ‚Üí manter status final

### Resolu√ß√£o
- Se a√ß√£o foi conclu√≠da em chunk posterior ‚Üí marcar como ‚úÖ
- Se a√ß√£o foi cancelada/substitu√≠da ‚Üí remover ou marcar

## ESTRUTURA OBRIGAT√ìRIA

# üìã Plano de A√ß√£o

## üìå Resumo Executivo
- **Total de a√ß√µes identificadas**: [X]
- **Compromissos firmes**: [Y]
- **Pendentes de confirma√ß√£o**: [Z]

## üî¥ A√ß√µes de Alta Prioridade

| # | A√ß√£o | Respons√°vel | Prazo | Status | Evid√™ncia |
|---|------|-------------|-------|--------|-----------|
| 1 | [descri√ß√£o] | [nome] | [data] | ‚è≥/‚úÖ | "[cita√ß√£o]" |

### Depend√™ncias
- A1 ‚Üí A3 (A3 depende de A1)

## üü° A√ß√µes de M√©dia Prioridade

| # | A√ß√£o | Respons√°vel | Prazo | Status | Evid√™ncia |
|---|------|-------------|-------|--------|-----------|
| ... |

## üü¢ A√ß√µes de Baixa Prioridade / Sugest√µes

| # | Sugest√£o | Poss√≠vel Respons√°vel | Contexto |
|---|----------|---------------------|----------|
| ... |

## ‚è≥ Aguardando Confirma√ß√£o
| Solicita√ß√£o | Para Quem | Desde |
|-------------|-----------|-------|
| [descri√ß√£o] | [nome] | [parte/momento] |

## ‚ö†Ô∏è Riscos Identificados
- [A√ß√£o X sem respons√°vel definido]
- [Prazo Y pode conflitar com Z]

## üìÖ Linha do Tempo de Entregas
```
Semana 1: [a√ß√µes]
Semana 2: [a√ß√µes]
```"""
    }
}


def get_chunk_settings(model: str) -> Dict:
    """Get chunk size settings based on model."""
    return MODEL_CHUNK_SIZES.get(model, {"size": DEFAULT_CHUNK_SIZE, "overlap": DEFAULT_CHUNK_OVERLAP})


def create_chunks(text: str, model: str = "gpt-4-turbo") -> List[Dict]:
    """Divide o texto em chunks com overlap, baseado no modelo."""
    settings = get_chunk_settings(model)
    chunk_size = settings["size"]
    chunk_overlap = settings["overlap"]
    
    chunks = []
    text_length = len(text)
    
    if text_length <= chunk_size:
        return [{
            "index": 0,
            "start": 0,
            "end": text_length,
            "text": text
        }]
    
    start = 0
    chunk_index = 0
    
    while start < text_length:
        end = min(start + chunk_size, text_length)
        
        chunks.append({
            "index": chunk_index,
            "start": start,
            "end": end,
            "text": text[start:end]
        })
        
        if end >= text_length:
            break
            
        start = start + chunk_size - chunk_overlap
        chunk_index += 1
    
    return chunks


def run_async(coro):
    """Helper to run async code in sync context."""
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    return loop.run_until_complete(coro)


try:
    from app.core.celery_app import celery_app
    
    if celery_app:
        @celery_app.task(bind=True, max_retries=MAX_RETRIES)
        def process_pii_analysis_chunked(self, analysis_id: str):
            """
            Processa uma an√°lise de PII dividida em chunks com rate limit handling.
            """
            db = SessionLocal()
            chunk_times = []
            
            try:
                analysis = db.query(PIIAnalysis).filter(PIIAnalysis.id == analysis_id).first()
                if not analysis:
                    logger.error(f"Analysis {analysis_id} not found")
                    return {"error": "Analysis not found"}
                
                job = db.query(PIIProcessingJob).filter(PIIProcessingJob.id == analysis.job_id).first()
                if not job:
                    logger.error(f"Job not found for analysis {analysis_id}")
                    return {"error": "Job not found"}
                
                model = analysis.llm_model or "gpt-4-turbo"
                chat_text = job.masked_chat_text or ""
                chunks = create_chunks(chat_text, model)
                total_chunks = len(chunks)
                logger.info(f"Created {total_chunks} chunks for model {model} (chunk_size: {get_chunk_settings(model)['size']})")
                
                analysis.is_chunked = True
                analysis.total_chunks = str(total_chunks)
                analysis.completed_chunks = "0"
                analysis.failed_chunks = "0"
                analysis.status = "processing"
                analysis.started_at = datetime.utcnow()
                analysis.is_paused = False
                analysis.pause_reason = None
                db.commit()
                
                from sqlalchemy import cast, Integer
                existing_chunks = db.query(PIIAnalysisChunk).filter(
                    PIIAnalysisChunk.analysis_id == analysis.id
                ).count()
                
                if existing_chunks == 0:
                    for chunk_data in chunks:
                        chunk = PIIAnalysisChunk(
                            analysis_id=analysis.id,
                            chunk_index=str(chunk_data["index"]),
                            total_chunks=str(total_chunks),
                            start_char=str(chunk_data["start"]),
                            end_char=str(chunk_data["end"]),
                            status="pending",
                            max_retries=str(MAX_RETRIES)
                        )
                        db.add(chunk)
                    db.commit()
                
                llm_service = get_llm_service()
                task_type = analysis.task_type
                delay_between = int(analysis.delay_between_chunks or "2")
                
                chunk_records = db.query(PIIAnalysisChunk).filter(
                    PIIAnalysisChunk.analysis_id == analysis.id,
                    PIIAnalysisChunk.status.in_(["pending", "processing", "failed"])
                ).order_by(cast(PIIAnalysisChunk.chunk_index, Integer)).all()
                
                for chunk_record in chunk_records:
                    i = int(chunk_record.chunk_index)
                    chunk_text = chat_text[int(chunk_record.start_char):int(chunk_record.end_char)]
                    
                    prompt_template = TASK_PROMPTS.get(task_type, {}).get("chunk", "")
                    chunk_prompt = prompt_template.format(
                        chunk_num=i + 1,
                        total_chunks=total_chunks
                    )
                    
                    full_prompt = f"""{chunk_prompt}

Conversa (Parte {i + 1} de {total_chunks}):
{chunk_text}

Resposta:"""
                    
                    chunk_record.prompt = full_prompt
                    chunk_record.status = "processing"
                    chunk_record.started_at = datetime.utcnow()
                    db.commit()
                    
                    retry_count = int(chunk_record.retry_count or "0")
                    max_retries = int(chunk_record.max_retries or str(MAX_RETRIES))
                    success = False
                    last_error = None
                    chunk_start_time = time.time()
                    
                    while retry_count < max_retries and not success:
                        try:
                            response = run_async(llm_service.analyze(
                                prompt=full_prompt,
                                model=model,
                                temperature=0.7,
                                max_tokens=1000
                            ))
                            
                            chunk_end_time = time.time()
                            processing_time_ms = int((chunk_end_time - chunk_start_time) * 1000)
                            chunk_times.append(processing_time_ms)
                            
                            chunk_record.llm_response = response
                            chunk_record.status = "completed"
                            chunk_record.retry_count = str(retry_count)
                            chunk_record.completed_at = datetime.utcnow()
                            chunk_record.processing_time_ms = str(processing_time_ms)
                            chunk_record.result_data = {"response": response[:500] if response else None}
                            chunk_record.error_message = None
                            chunk_record.error_code = None
                            
                            analysis.completed_chunks = str(int(analysis.completed_chunks or "0") + 1)
                            update_analysis_timing(db, analysis, chunk_times)
                            db.commit()
                            success = True
                            
                            remaining_chunks = db.query(PIIAnalysisChunk).filter(
                                PIIAnalysisChunk.analysis_id == analysis.id,
                                PIIAnalysisChunk.status.in_(["pending", "processing"])
                            ).count()
                            
                            if remaining_chunks > 0:
                                time.sleep(delay_between)
                                
                        except Exception as e:
                            retry_count += 1
                            last_error = e
                            error_str = str(e)
                            
                            rate_info = extract_rate_limit_info(error_str)
                            
                            chunk_record.retry_count = str(retry_count)
                            chunk_record.last_retry_at = datetime.utcnow()
                            chunk_record.error_message = error_str[:500]
                            
                            if rate_info["is_rate_limit"]:
                                chunk_record.error_code = "RATE_LIMIT"
                                chunk_record.rate_limit_delay_s = str(rate_info["wait_seconds"])
                                
                                analysis.pause_reason = f"Rate limit atingido. Aguardando {rate_info['wait_seconds']}s..."
                                analysis.rate_limit_wait_until = datetime.utcnow() + timedelta(seconds=rate_info["wait_seconds"])
                                db.commit()
                                
                                logger.warning(f"Rate limit hit on chunk {i}, waiting {rate_info['wait_seconds']}s")
                                time.sleep(rate_info["wait_seconds"])
                                
                                analysis.pause_reason = None
                                analysis.rate_limit_wait_until = None
                                db.commit()
                            else:
                                chunk_record.error_code = "UNKNOWN"
                                db.commit()
                                
                                logger.error(f"Error processing chunk {i} (attempt {retry_count}/{max_retries}): {e}")
                                
                                if retry_count < max_retries:
                                    time.sleep(RETRY_DELAY)
                    
                    if not success:
                        chunk_record.status = "failed"
                        chunk_record.error_message = str(last_error)[:500]
                        analysis.failed_chunks = str(int(analysis.failed_chunks or "0") + 1)
                        db.commit()
                        
                        rate_info = extract_rate_limit_info(str(last_error))
                        if rate_info["is_rate_limit"]:
                            analysis.is_paused = True
                            analysis.pause_reason = f"Pausado: rate limit ap√≥s {max_retries} tentativas. Considere usar GPT-3.5-turbo."
                            analysis.status = "paused"
                            db.commit()
                            return {
                                "status": "paused",
                                "reason": "rate_limit",
                                "analysis_id": str(analysis.id),
                                "failed_chunk": i,
                                "suggestion": "Use gpt-3.5-turbo for faster processing"
                            }
                
                failed_count = int(analysis.failed_chunks or "0")
                if failed_count > 0:
                    completed_count = int(analysis.completed_chunks or "0")
                    if completed_count > 0:
                        analysis.status = "partial"
                        analysis.pause_reason = f"{failed_count} chunks falharam. Pode continuar com outro modelo."
                        db.commit()
                        return {
                            "status": "partial",
                            "analysis_id": str(analysis.id),
                            "completed": completed_count,
                            "failed": failed_count
                        }
                
                consolidate_pii_analysis.delay(str(analysis.id))
                
                return {"status": "chunks_completed", "analysis_id": str(analysis.id)}
                
            except Exception as e:
                logger.error(f"Error in process_pii_analysis_chunked: {e}")
                if analysis:
                    analysis.status = "failed"
                    analysis.llm_response = f"Erro: {str(e)}"
                    db.commit()
                raise
            finally:
                db.close()
        
        
        @celery_app.task(bind=True, max_retries=MAX_RETRIES)
        def consolidate_pii_analysis(self, analysis_id: str):
            """
            Consolida os resultados dos chunks em uma an√°lise final.
            """
            db = SessionLocal()
            
            try:
                analysis = db.query(PIIAnalysis).filter(PIIAnalysis.id == analysis_id).first()
                if not analysis:
                    logger.error(f"Analysis {analysis_id} not found")
                    return {"error": "Analysis not found"}
                
                from sqlalchemy import cast, Integer
                chunks = db.query(PIIAnalysisChunk).filter(
                    PIIAnalysisChunk.analysis_id == analysis.id,
                    PIIAnalysisChunk.status == "completed"
                ).order_by(cast(PIIAnalysisChunk.chunk_index, Integer)).all()
                
                if not chunks:
                    analysis.status = "failed"
                    analysis.llm_response = "Nenhum chunk processado com sucesso"
                    db.commit()
                    return {"error": "No completed chunks"}
                
                chunk_results = "\n\n---\n\n".join([
                    f"Parte {c.chunk_index}: {c.llm_response}" for c in chunks
                ])
                
                task_type = analysis.task_type
                consolidate_template = TASK_PROMPTS.get(task_type, {}).get("consolidate", "")
                
                consolidate_prompt = consolidate_template.format(chunk_results=chunk_results)
                
                llm_service = get_llm_service()
                model = analysis.llm_model or "gpt-4-turbo"
                
                try:
                    final_response = run_async(llm_service.analyze(
                        prompt=consolidate_prompt,
                        model=model,
                        temperature=0.7,
                        max_tokens=2000
                    ))
                    
                    analysis.consolidated_response = final_response
                    analysis.llm_response = final_response
                    analysis.status = "completed"
                    db.commit()
                    
                    return {"status": "completed", "analysis_id": str(analysis.id)}
                    
                except Exception as e:
                    logger.error(f"Error consolidating analysis: {e}")
                    analysis.status = "failed"
                    analysis.llm_response = f"Erro ao consolidar: {str(e)}"
                    db.commit()
                    raise
                    
            finally:
                db.close()
                
        logger.info("‚úÖ PII Celery tasks registered successfully")
        
except Exception as e:
    logger.warning(f"‚ö†Ô∏è Could not register PII Celery tasks: {e}")


def process_pii_analysis_sync(analysis_id: str, db=None) -> Dict:
    """
    Vers√£o s√≠ncrona do processamento de an√°lise PII em chunks.
    Usado quando Celery n√£o est√° dispon√≠vel.
    """
    close_db = False
    if db is None:
        db = SessionLocal()
        close_db = True
    
    try:
        analysis = db.query(PIIAnalysis).filter(PIIAnalysis.id == analysis_id).first()
        if not analysis:
            return {"error": "Analysis not found"}
        
        job = db.query(PIIProcessingJob).filter(PIIProcessingJob.id == analysis.job_id).first()
        if not job:
            return {"error": "Job not found"}
        
        chat_text = job.masked_chat_text or ""
        chunks = create_chunks(chat_text)
        total_chunks = len(chunks)
        
        analysis.is_chunked = True
        analysis.total_chunks = str(total_chunks)
        analysis.completed_chunks = "0"
        analysis.status = "processing"
        db.commit()
        
        for chunk_data in chunks:
            chunk = PIIAnalysisChunk(
                analysis_id=analysis.id,
                chunk_index=str(chunk_data["index"]),
                total_chunks=str(total_chunks),
                start_char=str(chunk_data["start"]),
                end_char=str(chunk_data["end"]),
                status="pending"
            )
            db.add(chunk)
        db.commit()
        
        llm_service = get_llm_service()
        model = analysis.llm_model or "gpt-4-turbo"
        task_type = analysis.task_type
        
        from sqlalchemy import cast, Integer
        chunk_records = db.query(PIIAnalysisChunk).filter(
            PIIAnalysisChunk.analysis_id == analysis.id
        ).order_by(cast(PIIAnalysisChunk.chunk_index, Integer)).all()
        
        for i, chunk_record in enumerate(chunk_records):
            chunk_text = chat_text[int(chunk_record.start_char):int(chunk_record.end_char)]
            
            prompt_template = TASK_PROMPTS.get(task_type, {}).get("chunk", "")
            chunk_prompt = prompt_template.format(
                chunk_num=i + 1,
                total_chunks=total_chunks
            )
            
            full_prompt = f"""{chunk_prompt}

Conversa (Parte {i + 1} de {total_chunks}):
{chunk_text}

Resposta:"""
            
            chunk_record.prompt = full_prompt
            chunk_record.status = "processing"
            db.commit()
            
            retry_count = 0
            success = False
            last_error = None
            
            while retry_count < MAX_RETRIES and not success:
                try:
                    response = run_async(llm_service.analyze(
                        prompt=full_prompt,
                        model=model,
                        temperature=0.7,
                        max_tokens=1000
                    ))
                    
                    chunk_record.llm_response = response
                    chunk_record.status = "completed"
                    chunk_record.retry_count = str(retry_count)
                    
                    analysis.completed_chunks = str(int(analysis.completed_chunks or "0") + 1)
                    db.commit()
                    success = True
                    
                    if i < len(chunk_records) - 1:
                        time.sleep(2)
                        
                except Exception as e:
                    retry_count += 1
                    last_error = e
                    logger.error(f"Error processing chunk {i} (attempt {retry_count}/{MAX_RETRIES}): {e}")
                    chunk_record.retry_count = str(retry_count)
                    db.commit()
                    
                    if retry_count < MAX_RETRIES:
                        time.sleep(RETRY_DELAY)
            
            if not success:
                chunk_record.status = "failed"
                chunk_record.error_message = str(last_error)
                db.commit()
        
        completed_chunks = db.query(PIIAnalysisChunk).filter(
            PIIAnalysisChunk.analysis_id == analysis.id,
            PIIAnalysisChunk.status == "completed"
        ).order_by(cast(PIIAnalysisChunk.chunk_index, Integer)).all()
        
        if not completed_chunks:
            analysis.status = "failed"
            analysis.llm_response = "Nenhum chunk processado com sucesso"
            db.commit()
            return {"error": "No completed chunks"}
        
        chunk_results = "\n\n---\n\n".join([
            f"Parte {c.chunk_index}: {c.llm_response}" for c in completed_chunks
        ])
        
        consolidate_template = TASK_PROMPTS.get(task_type, {}).get("consolidate", "")
        consolidate_prompt = consolidate_template.format(chunk_results=chunk_results)
        
        try:
            final_response = run_async(llm_service.analyze(
                prompt=consolidate_prompt,
                model=model,
                temperature=0.7,
                max_tokens=2000
            ))
            
            analysis.consolidated_response = final_response
            analysis.llm_response = final_response
            analysis.status = "completed"
            db.commit()
            
            return {"status": "completed", "analysis_id": str(analysis.id)}
            
        except Exception as e:
            logger.error(f"Error consolidating analysis: {e}")
            analysis.status = "failed"
            analysis.llm_response = f"Erro ao consolidar: {str(e)}"
            db.commit()
            return {"error": str(e)}
            
    finally:
        if close_db:
            db.close()
